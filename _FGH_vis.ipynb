{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b998746-5188-44d7-a396-a34aaaa86662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binlin/miniconda3/envs/dvt/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/home/binlin/miniconda3/envs/dvt/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/home/binlin/miniconda3/envs/dvt/lib/python3.10/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import datetime as dt\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import chain\n",
    "import imageio\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from transform import RandomResizedCropFlip\n",
    "from vit_wrapper import PretrainedViTWrapper\n",
    "from single_image_dataset import SingleImageDataset\n",
    "\n",
    "from neural_feature_field import NeuralFeatureField\n",
    "from offline_denoiser import SingleImageDenoiser\n",
    "from visualization_tools import visualize_offline_denoised_samples\n",
    "\n",
    "import misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5149de1e-0662-4bff-bdc0-04da7bb71840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0176de8-bae1-49fb-a974-b417686a4014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "args = types.SimpleNamespace(\n",
    "    model=\"vit_base_patch14_dinov2.lvd142m\",\n",
    "    input_size=(518, 518),\n",
    "    stride_size=14,\n",
    "    layer_depth_ratio=1.0,\n",
    "    img_path=\"demo/assets/demo/cat.jpg\",\n",
    "    dtype=\"float32\",\n",
    "    data_root=None,#\"./features_FGH\",\n",
    "    save_root=None,\n",
    "    start_idx=0,\n",
    "    num_imgs=1,\n",
    "    num_views=768,\n",
    "    num_iters=25_000,\n",
    "    warmup_iters=100,\n",
    "    n_levels=16,\n",
    "    freeze_shared_artifacts_after=0.5,\n",
    "    lr=0.01,\n",
    "    min_lr=0.001,\n",
    "    weight_decay=1e-5,\n",
    "    extract_bsz=32,\n",
    "    pixel_bsz=2048,\n",
    "    output_dir=\"./work_dirs/demo\",\n",
    "    num_vis_samples=5,\n",
    "    vis_freq=100,\n",
    "    seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dbdfd6a-70df-4aeb-9b7c-25de8588ab8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "vit = PretrainedViTWrapper(model_identifier=args.model, stride=args.stride_size)\n",
    "vit = vit.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a4512ae-5262-4e94-92f6-7fa56fa5e728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_index: 11\n",
      "patch spatial size: (37, 37)\n",
      "feat_dim: 768\n",
      "dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# 4 parameters computed by specific ViT model\n",
    "layer_index = int(args.layer_depth_ratio * vit.last_layer_index)\n",
    "print(f\"layer_index: {layer_index}\")\n",
    "pos_h = (args.input_size[0] - vit.patch_size) // args.stride_size + 1\n",
    "pos_w = (args.input_size[1] - vit.patch_size) // args.stride_size + 1\n",
    "print(f\"patch spatial size: {pos_h, pos_w}\")\n",
    "feat_dim = vit.n_output_dims\n",
    "print(f\"feat_dim: {feat_dim}\")\n",
    "args.layer_index = layer_index\n",
    "args.feat_dim = vit.n_output_dims\n",
    "args.noise_map_height = pos_h\n",
    "args.noise_map_width = pos_w\n",
    "\n",
    "\n",
    "if not isinstance(args.dtype, torch.dtype):\n",
    "    args.dtype = torch.float32 if args.dtype == \"float32\" else torch.bfloat16\n",
    "print(f\"dtype: {args.dtype}\")\n",
    "\n",
    "# normalizer and denormalizer from the model\n",
    "normalizer = vit.transformation.transforms[-1]\n",
    "assert isinstance(normalizer, transforms.Normalize), \"last transform must be norm\"\n",
    "denormalizer = transforms.Normalize(\n",
    "    mean=[-m / s for m, s in zip(normalizer.mean, normalizer.std)],\n",
    "    std=[1 / s for s in normalizer.std],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fcdd7e1-f7e3-40c8-ac3c-fa361173e4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder for the data\n",
    "num_samples = args.num_views + 1  # + 1 for the original image\n",
    "global_pixel_coords = torch.zeros(\n",
    "    (num_samples, pos_h, pos_w, 2),\n",
    "    dtype=args.dtype,\n",
    "    device=device,\n",
    ")\n",
    "views = torch.zeros(\n",
    "    (num_samples, 3, args.input_size[0], args.input_size[1]),\n",
    "    dtype=args.dtype,\n",
    "    device=device,\n",
    ")\n",
    "vit_features = torch.zeros(\n",
    "    (num_samples, pos_h, pos_w, vit.n_output_dims),\n",
    "    dtype=args.dtype,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21a2eb20-4cf6-41ca-977c-30909d42b288",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SingleImageDataset(\n",
    "    size=args.input_size,\n",
    "    base_transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(args.input_size),\n",
    "            transforms.ToTensor(),\n",
    "            normalizer,\n",
    "        ]\n",
    "    ),\n",
    "    final_transform=RandomResizedCropFlip(\n",
    "        size=args.input_size,\n",
    "        horizontal_flip=True,\n",
    "        scale=(0.1, 0.5),\n",
    "        patch_size=vit.patch_size,\n",
    "        stride=args.stride_size,\n",
    "    ),\n",
    "    num_views=args.num_views,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0be0888-0aa8-4030-8615-ddbdf7a48598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_patch_coordinates(height, width, start=-1, end=1):\n",
    "    patch_y, patch_x = torch.linspace(start, end, height), torch.linspace(start, end, width)\n",
    "    patch_y, patch_x = torch.meshgrid(patch_y, patch_x, indexing=\"ij\")\n",
    "    patch_coordinates = torch.stack([patch_x, patch_y], dim=-1)\n",
    "    return patch_coordinates\n",
    "\n",
    "def denoise_an_image(\n",
    "    args,\n",
    "    all_raw_features: torch.Tensor,\n",
    "    all_pixel_coords: torch.Tensor,\n",
    "    all_transformed_views: torch.Tensor,\n",
    "    device: torch.device,\n",
    "    denormalizer: transforms.Normalize = None,\n",
    "    identifier: str = '',\n",
    "    # img_pth: str = None,\n",
    "    should_save_vis: bool = False,\n",
    "):\n",
    "    # ---- build the models and optimizer ---- #\n",
    "    denoiser = SingleImageDenoiser(\n",
    "        noise_map_height=args.noise_map_height,\n",
    "        noise_map_width=args.noise_map_width,\n",
    "        feat_dim=args.feat_dim,\n",
    "        layer_index=args.layer_index,\n",
    "    ).to(device)\n",
    "    # ---- build a neural field ----#\n",
    "    neural_field = NeuralFeatureField(feat_dim=args.feat_dim, n_levels=args.n_levels)\n",
    "    neural_field = neural_field.to(device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        chain(denoiser.parameters(), neural_field.parameters()),\n",
    "        lr=args.lr,\n",
    "        eps=1e-15,\n",
    "        weight_decay=args.weight_decay,\n",
    "        betas=(0.9, 0.99),\n",
    "    )\n",
    "    grad_scaler = torch.amp.GradScaler(\"cuda\", 2**10)\n",
    "\n",
    "    # ----- shared artifact (G) coordinates ----- #\n",
    "    shared_artifact_coords = make_patch_coordinates(args.noise_map_height, args.noise_map_width)\n",
    "    shared_artifact_coords = shared_artifact_coords.to(device)\n",
    "    num_views = all_raw_features.shape[0]\n",
    "    batched_shared_artifact_coords = shared_artifact_coords.unsqueeze(0).repeat(num_views, 1, 1, 1)\n",
    "    batched_shared_artifact_coords = batched_shared_artifact_coords.reshape(-1, 2)\n",
    "\n",
    "    batched_raw_features = all_raw_features.reshape(-1, all_raw_features.shape[-1])\n",
    "    batched_pixel_coordinates = all_pixel_coords.reshape(-1, 2)\n",
    "\n",
    "    \n",
    "    \n",
    "    for step in range(args.num_iters):\n",
    "        step_display = step + 1\n",
    "        denoiser.train()\n",
    "        neural_field.train()\n",
    "        if step > int(args.freeze_shared_artifacts_after * args.num_iters):\n",
    "            denoiser.stop_shared_artifacts_grad()\n",
    "            # if denoiser.residual_predictor_start == False:\n",
    "            #     save_FGH_full_snapshot(neural_field=neural_field,\n",
    "            #            denoiser=denoiser,\n",
    "            #            img_name=identifier,\n",
    "            #            output_dir=\"FGH1210_beforeH\",\n",
    "            #            args=args)\n",
    "            denoiser.start_residual_predictor()\n",
    "        random_pixel_indices = np.random.randint(0, batched_raw_features.shape[0], args.pixel_bsz)\n",
    "        raw_features = batched_raw_features[random_pixel_indices]\n",
    "        shared_artifact_coords = batched_shared_artifact_coords[random_pixel_indices]\n",
    "        pixel_coordinates = batched_pixel_coordinates[random_pixel_indices]\n",
    "        misc.adjust_learning_rate(optimizer, step, args)\n",
    "        with torch.autocast(device, dtype=args.dtype, enabled=args.dtype != torch.float32):\n",
    "            output = denoiser(\n",
    "                raw_vit_outputs=raw_features,\n",
    "                global_pixel_coords=pixel_coordinates,\n",
    "                neural_field=neural_field,\n",
    "                shared_artifact_coords=shared_artifact_coords,\n",
    "                return_visualization=False,\n",
    "            )\n",
    "            loss = output[\"loss\"]\n",
    "        optimizer.zero_grad()\n",
    "        grad_scaler.scale(loss).backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step_display % 1000 == 0:# or step == args.num_iters - 1:\n",
    "            print(\n",
    "                f\"Step {step}/{args.num_iters}: \"\n",
    "                f\"Loss = {loss.item():.4f}, \"\n",
    "                f\"Patch Loss = {output['patch_l2_loss'].item():.4f}, \"\n",
    "                f\"CosSim Loss = {output['cosine_similarity_loss'].item():.4f}, \"\n",
    "                f\"Residual Loss = {output['residual_loss'].item() if 'residual_loss' in output else 0:.4f}, \"\n",
    "                f\"Residual Sparsity Loss = {output['residual_sparsity_loss'].item() if 'residual_sparsity_loss' in output else 0:.4f}, \"\n",
    "                f\"LR = {optimizer.param_groups[0]['lr']:.4f}\"\n",
    "            )\n",
    "    if should_save_vis:\n",
    "        vis_indices = np.random.randint(0, num_views, args.num_vis_samples)\n",
    "        vis_indices = np.concatenate([vis_indices, [-1]])\n",
    "        train_pca_samples, pred_full_denoised_features = visualize_offline_denoised_samples(\n",
    "            denoiser=denoiser,\n",
    "            neural_field=neural_field,\n",
    "            raw_features=all_raw_features[vis_indices],\n",
    "            coord=all_pixel_coords[vis_indices],\n",
    "            patch_images=all_transformed_views[vis_indices],\n",
    "            device=device,\n",
    "            denormalizer=denormalizer,\n",
    "            dtype=args.dtype,\n",
    "        )\n",
    "        os.makedirs(f\"{args.output_dir}/visualization\", exist_ok=True)\n",
    "        # img_name = os.path.basename(img_pth)\n",
    "        imageio.imsave(f\"{args.output_dir}/visualization/{identifier}.jpg\", train_pca_samples)\n",
    "        \n",
    "        print(f\"Saved visualization to {args.output_dir}/visualization/{identifier}\")\n",
    "    else:\n",
    "        pred_full_denoised_features = None\n",
    "    pred_full_denoised_features = None\n",
    "    # if args.data_root is not None:\n",
    "    #     if pred_full_denoised_features is None:\n",
    "    #         with torch.no_grad():\n",
    "    #             output = denoiser(\n",
    "    #                 raw_vit_outputs=all_raw_features[-1:],\n",
    "    #                 global_pixel_coords=all_pixel_coords[-1:],\n",
    "    #                 neural_field=neural_field,\n",
    "    #                 return_visualization=True,\n",
    "    #             )\n",
    "    #         pred_full_denoised_features = output[\"denoised_feats\"].float().detach().cpu().numpy()\n",
    "    #     raw_feat_dir = f\"{args.save_root}/raw_features/{args.model}/\"\n",
    "    #     denoised_feat_dir = f\"{args.save_root}/denoised_features/{args.model}/\"\n",
    "    #     # img_extention = os.path.splitext(img_pth)[1]\n",
    "    #     # raw_feat_save_path = img_pth.replace(args.data_root, raw_feat_dir).replace(\n",
    "    #     #     img_extention, \".npy\"\n",
    "    #     # )\n",
    "    #     # denoised_feat_save_path = img_pth.replace(args.data_root, denoised_feat_dir).replace(\n",
    "    #     #     img_extention, \".npy\"\n",
    "    #     # )\n",
    "    #     raw_feat_save_path = os.path.join(args.data_root, \"rwa_feat\", identifier+\".npy\")\n",
    "    #     denoised_feat_save_path = os.path.join(args.data_root, \"denoised_feat\", identifier+\".npy\")\n",
    "    #     os.makedirs(os.path.dirname(raw_feat_save_path), exist_ok=True)\n",
    "    #     os.makedirs(os.path.dirname(denoised_feat_save_path), exist_ok=True)\n",
    "    #     np.save(raw_feat_save_path, all_raw_features[-1].float().detach().cpu().numpy())\n",
    "    #     np.save(denoised_feat_save_path, pred_full_denoised_features)\n",
    "    #     print(\n",
    "    #         f\"Saved denoised features to {denoised_feat_save_path} and raw features to {raw_feat_save_path}\"\n",
    "    #     )\n",
    "    # save_FGH_full_snapshot(neural_field=neural_field,\n",
    "    #                        denoiser=denoiser,\n",
    "    #                        img_name=identifier,\n",
    "    #                        output_dir=\"FGH1210\",\n",
    "    #                        args=args)\n",
    "    \n",
    "    # del denoiser, neural_field, optimizer\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "    return {\"denoiser\": denoiser, \n",
    "            \"neural_field\": neural_field\n",
    "           }\n",
    "\n",
    "# def save_FGH_full_snapshot(neural_field, denoiser, img_name, output_dir, args):\n",
    "#     base_name = os.path.splitext(img_name)[0]\n",
    "#     # model_dir = os.path.join(args.output_dir, \"models\")\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         torch.save(\n",
    "#             {\n",
    "#                 \"F_neural_field\": neural_field.state_dict(),\n",
    "#                 \"G_shared_artifacts\": denoiser.shared_artifacts.detach().cpu(),\n",
    "#                 \"H_residual_predictor\": (\n",
    "#                     denoiser.residual_predictor.state_dict()\n",
    "#                     if hasattr(denoiser, \"residual_predictor\")\n",
    "#                     and denoiser.residual_predictor is not None\n",
    "#                     else None\n",
    "#                 ),\n",
    "#                 \"args\": vars(args),\n",
    "#             },\n",
    "#             os.path.join(output_dir, f\"{base_name}.pt\")\n",
    "#         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "123a6e10-f4e0-497b-aeed-203612ba7610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting features: 100%|██████████████████████| 24/24 [00:15<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 999/25000: Loss = 0.8172, Patch Loss = 0.6908, CosSim Loss = 0.1264, Residual Loss = 0.0000, Residual Sparsity Loss = 0.0000, LR = 0.0100\n",
      "Step 1999/25000: Loss = 0.7469, Patch Loss = 0.6329, CosSim Loss = 0.1141, Residual Loss = 0.0000, Residual Sparsity Loss = 0.0000, LR = 0.0099\n",
      "Step 2999/25000: Loss = 0.7094, Patch Loss = 0.6009, CosSim Loss = 0.1086, Residual Loss = 0.0000, Residual Sparsity Loss = 0.0000, LR = 0.0097\n",
      "Step 3999/25000: Loss = 0.7351, Patch Loss = 0.6223, CosSim Loss = 0.1128, Residual Loss = 0.0000, Residual Sparsity Loss = 0.0000, LR = 0.0095\n",
      "Step 4999/25000: Loss = 0.7084, Patch Loss = 0.6010, CosSim Loss = 0.1074, Residual Loss = 0.0000, Residual Sparsity Loss = 0.0000, LR = 0.0092\n",
      "Step 5999/25000: Loss = 0.6716, Patch Loss = 0.5696, CosSim Loss = 0.1020, Residual Loss = 0.0000, Residual Sparsity Loss = 0.0000, LR = 0.0088\n",
      "Step 6999/25000: Loss = 0.6755, Patch Loss = 0.5730, CosSim Loss = 0.1025, Residual Loss = 0.0000, Residual Sparsity Loss = 0.0000, LR = 0.0084\n",
      "Step 7999/25000: Loss = 0.6915, Patch Loss = 0.5863, CosSim Loss = 0.1052, Residual Loss = 0.0000, Residual Sparsity Loss = 0.0000, LR = 0.0079\n",
      "Step 8999/25000: Loss = 0.6672, Patch Loss = 0.5655, CosSim Loss = 0.1017, Residual Loss = 0.0000, Residual Sparsity Loss = 0.0000, LR = 0.0074\n",
      "Step 9999/25000: Loss = 0.6445, Patch Loss = 0.5465, CosSim Loss = 0.0980, Residual Loss = 0.0000, Residual Sparsity Loss = 0.0000, LR = 0.0069\n",
      "Step 10999/25000: Loss = 0.6557, Patch Loss = 0.5563, CosSim Loss = 0.0994, Residual Loss = 0.0000, Residual Sparsity Loss = 0.0000, LR = 0.0064\n",
      "Step 11999/25000: Loss = 0.6290, Patch Loss = 0.5331, CosSim Loss = 0.0959, Residual Loss = 0.0000, Residual Sparsity Loss = 0.0000, LR = 0.0058\n",
      "Step 12999/25000: Loss = 0.7015, Patch Loss = 0.5479, CosSim Loss = 0.0988, Residual Loss = 0.0548, Residual Sparsity Loss = 0.0000, LR = 0.0052\n",
      "Step 13999/25000: Loss = 0.5005, Patch Loss = 0.3882, CosSim Loss = 0.0676, Residual Loss = 0.0388, Residual Sparsity Loss = 0.0058, LR = 0.0047\n",
      "Step 14999/25000: Loss = 0.4543, Patch Loss = 0.3515, CosSim Loss = 0.0609, Residual Loss = 0.0351, Residual Sparsity Loss = 0.0067, LR = 0.0041\n",
      "Step 15999/25000: Loss = 0.4493, Patch Loss = 0.3475, CosSim Loss = 0.0600, Residual Loss = 0.0348, Residual Sparsity Loss = 0.0071, LR = 0.0036\n",
      "Step 16999/25000: Loss = 0.4515, Patch Loss = 0.3491, CosSim Loss = 0.0603, Residual Loss = 0.0349, Residual Sparsity Loss = 0.0071, LR = 0.0031\n",
      "Step 17999/25000: Loss = 0.4408, Patch Loss = 0.3406, CosSim Loss = 0.0589, Residual Loss = 0.0341, Residual Sparsity Loss = 0.0073, LR = 0.0026\n",
      "Step 18999/25000: Loss = 0.4171, Patch Loss = 0.3219, CosSim Loss = 0.0556, Residual Loss = 0.0322, Residual Sparsity Loss = 0.0074, LR = 0.0022\n",
      "Step 19999/25000: Loss = 0.4123, Patch Loss = 0.3181, CosSim Loss = 0.0551, Residual Loss = 0.0318, Residual Sparsity Loss = 0.0074, LR = 0.0019\n",
      "Step 20999/25000: Loss = 0.4077, Patch Loss = 0.3144, CosSim Loss = 0.0543, Residual Loss = 0.0314, Residual Sparsity Loss = 0.0076, LR = 0.0016\n",
      "Step 21999/25000: Loss = 0.3893, Patch Loss = 0.3004, CosSim Loss = 0.0514, Residual Loss = 0.0300, Residual Sparsity Loss = 0.0075, LR = 0.0013\n",
      "Step 22999/25000: Loss = 0.3800, Patch Loss = 0.2925, CosSim Loss = 0.0505, Residual Loss = 0.0292, Residual Sparsity Loss = 0.0078, LR = 0.0011\n",
      "Step 23999/25000: Loss = 0.3759, Patch Loss = 0.2893, CosSim Loss = 0.0499, Residual Loss = 0.0289, Residual Sparsity Loss = 0.0078, LR = 0.0010\n",
      "Step 24999/25000: Loss = 0.3752, Patch Loss = 0.2883, CosSim Loss = 0.0502, Residual Loss = 0.0288, Residual Sparsity Loss = 0.0079, LR = 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/binlin/miniconda3/envs/dvt/lib/python3.10/site-packages/torch_kmeans/clustering/kmeans.py:530: UserWarning: standard k-means should use a non-inverted distance measure.\n",
      "  warn(\"standard k-means should use a non-inverted distance measure.\")\n",
      "/home/binlin/Coding/dvt_stage1_analysis/layout.py:105: UserWarning: Using a non-tuple sequence for multidimensional indexing is deprecated and will be changed in pytorch 2.9; use x[tuple(seq)] instead of x[seq]. In pytorch 2.9 this will be interpreted as tensor index, x[torch.tensor(seq)], which will result either in an error or a different result (Triggered internally at /pytorch/torch/csrc/autograd/python_variable_indexing.cpp:355.)\n",
      "  result[selector] = overlay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualization to ./work_dirs/demo/visualization/2012_000521\n",
      "Training time elapse = 0:02:19.633496\n"
     ]
    }
   ],
   "source": [
    "tik = dt.datetime.now()\n",
    "autocast_ctx = torch.autocast(\"cuda\", dtype=args.dtype, enabled=args.dtype != torch.float32)\n",
    "\n",
    "np.random.seed(42)\n",
    "voc2012_path = \"\" # e.g \"YOUR_PATH/VOCdevkit/VOC2012/JPEGImages\"\n",
    "img_filenames = os.listdir(voc2012_path)\n",
    "img_filenames = sorted([fnm for fnm in img_filenames if fnm[-4:]==\".jpg\"])\n",
    "# selected_filenames = np.random.choice(\n",
    "#     img_filenames,\n",
    "#     size=5_000,\n",
    "#     replace=False\n",
    "# )\n",
    "# selected_filenames = np.random.permutation(img_filenames)[::-1][:2000]\n",
    "selected_filenames = np.random.permutation(img_filenames)[:1]\n",
    "\n",
    "img_paths = [os.path.join(voc2012_path, fnm) for fnm in selected_filenames]\n",
    "\n",
    "for img_path in img_paths:\n",
    "    # img_path = img_paths[1]\n",
    "    dataset.set_image(img_path)\n",
    "    \n",
    "    \n",
    "    \n",
    "    collect_loader = torch.utils.data.DataLoader(dataset, args.extract_bsz, num_workers=8)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    pbar = tqdm(collect_loader, total=len(collect_loader), desc=\"Collecting features\")\n",
    "    for i, data in enumerate(pbar):\n",
    "        with torch.no_grad(), autocast_ctx:\n",
    "            batch_vit_features = vit.get_intermediate_layers(\n",
    "                data[\"transformed_view\"].to(device),\n",
    "                n=[layer_index],\n",
    "                reshape=True,\n",
    "            )[-1]\n",
    "            # (B, C, H, W) -> (B, H, W, C)\n",
    "            batch_vit_features = batch_vit_features.permute(0, 2, 3, 1)\n",
    "            batch_pixel_coords = data[\"pixel_coords\"].to(device)\n",
    "            batch_views = data[\"transformed_view\"].to(device)\n",
    "            slicer = slice(i * args.extract_bsz, i * args.extract_bsz + batch_views.shape[0])\n",
    "            global_pixel_coords[slicer] = batch_pixel_coords\n",
    "            views[slicer] = batch_views\n",
    "            vit_features[slicer] = batch_vit_features\n",
    "    with torch.no_grad(), autocast_ctx:\n",
    "        original_vit_features = vit.get_intermediate_layers(\n",
    "            data[\"full_image\"][:1].to(device), n=[layer_index], reshape=True\n",
    "        )[-1]\n",
    "        # (B, C, H, W) -> (B, H, W, C)\n",
    "        original_vit_features = original_vit_features.permute(0, 2, 3, 1)\n",
    "    global_pixel_coords[-1] = make_patch_coordinates(pos_h, pos_w, start=0, end=1)\n",
    "    views[-1] = data[\"full_image\"][0].to(device)\n",
    "    vit_features[-1] = original_vit_features[0]\n",
    "    \n",
    "    \n",
    "    denoised_results = denoise_an_image(\n",
    "        args,\n",
    "        all_raw_features=vit_features,\n",
    "        all_pixel_coords=global_pixel_coords,\n",
    "        all_transformed_views=views,\n",
    "        device=device,\n",
    "        # img_pth=filename,\n",
    "        identifier = img_path.split(\"/\")[-1].split(\".\")[0],\n",
    "        denormalizer=denormalizer,\n",
    "        should_save_vis=True#idx % args.vis_freq == 0,\n",
    "    )\n",
    "    tok = dt.datetime.now()\n",
    "    print(f\"Training time elapse = {tok-tik}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1e5d29-a6ca-492a-a736-0f3b5bfb8202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4d36b1-7627-4246-ab47-688d665bf342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dvt)",
   "language": "python",
   "name": "dvt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
